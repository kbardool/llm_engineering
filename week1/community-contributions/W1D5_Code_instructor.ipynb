{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5dc476-e3c9-49bd-934a-35dbe0d55b13",
   "metadata": {},
   "source": [
    "# End of week 1 exercise (with user input(question, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353fba18-a9b4-4ba8-be7e-f3e3c37521ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:32:49.672676Z",
     "iopub.status.busy": "2025-04-23T00:32:49.671630Z",
     "iopub.status.idle": "2025-04-23T00:32:50.577601Z",
     "shell.execute_reply": "2025-04-23T00:32:50.576705Z",
     "shell.execute_reply.started": "2025-04-23T00:32:49.672631Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2b859d-b3d2-41f7-8666-28ecde26e3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:32:50.872073Z",
     "iopub.status.busy": "2025-04-23T00:32:50.871379Z",
     "iopub.status.idle": "2025-04-23T00:32:50.876778Z",
     "shell.execute_reply": "2025-04-23T00:32:50.876078Z",
     "shell.execute_reply.started": "2025-04-23T00:32:50.872050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment and constants\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b2b694-11a1-4d2a-8e34-d1fb02617fa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:32:51.720611Z",
     "iopub.status.busy": "2025-04-23T00:32:51.720316Z",
     "iopub.status.idle": "2025-04-23T00:32:51.725193Z",
     "shell.execute_reply": "2025-04-23T00:32:51.724572Z",
     "shell.execute_reply.started": "2025-04-23T00:32:51.720592Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert coder with educational skills for beginners. \\\n",
    "You are able to explain, debbug or generate code in Python, R or bash, and to provide examples of use case if applicable. \\\n",
    "Please add references to relevant sources if available. If not, do not invent.\\n\"\n",
    "system_prompt += \"this is an example of a response:\"\n",
    "system_prompt += \"\"\"\n",
    "Sure! Hereâ€™s the explanation in plain text format, suitable for Markdown:\n",
    "\n",
    "# Explanation of the Code\n",
    "\n",
    "### Code:\n",
    "```python\n",
    "full_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Lambda Function:**\n",
    "   - The keyword `lambda` is used to create a small, one-line anonymous function (a function without a name).\n",
    "   - It takes two parameters: `first` (for the first name) and `last` (for the last name).\n",
    "\n",
    "2. **String Formatting (`f-string`):**\n",
    "   - `f'Full name: {first.title()} {last.title()}'` is a formatted string (f-string).\n",
    "   - It inserts the values of `first` and `last` into the string while applying `.title()` to capitalize the first letter of each name.\n",
    "\n",
    "3. **Assigning the Function:**\n",
    "   - The lambda function is assigned to the variable `full_name`, so we can use `full_name()` like a regular function.\n",
    "\n",
    "### How to Use It:\n",
    "Now, letâ€™s call this function and see what it does.\n",
    "\n",
    "```python\n",
    "print(full_name(\"john\", \"doe\"))\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "Full name: John Doe\n",
    "```\n",
    "\n",
    "### What Happens:\n",
    "- `\"john\"` becomes `\"John\"` (because `.title()` capitalizes the first letter).\n",
    "- `\"doe\"` becomes `\"Doe\"`.\n",
    "- The output is `\"Full name: John Doe\"`.\n",
    "\n",
    "### Summary:\n",
    "This is a simple way to create a function that formats a full name while ensuring proper capitalization. You could write the same function using `def` like this:\n",
    "\n",
    "```python\n",
    "def full_name(first, last):\n",
    "    return f'Full name: {first.title()} {last.title()}'\n",
    "```\n",
    "\n",
    "Both versions work the same way, but the `lambda` version is more compact.\n",
    "\n",
    "### Reference(s):\n",
    "To deepen your understanding of the code snippet involving Python's lambda functions here is a resource you might find helpful:\n",
    "\n",
    "Ref. **Python Lambda Functions:**\n",
    "   - The official Python documentation provides an in-depth explanation of lambda expressions, including their syntax and use cases.îˆ†\n",
    "     - [Lambda Expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)\n",
    "\n",
    "```\n",
    "You can copy and paste this into any Markdown file or viewer. Let me know if you need further modifications! ðŸ˜Š\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7225ab0-5ade-4c93-839c-3c80b0b23c37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:00.884723Z",
     "iopub.status.busy": "2025-04-23T00:33:00.884493Z",
     "iopub.status.idle": "2025-04-23T00:33:00.890516Z",
     "shell.execute_reply": "2025-04-23T00:33:00.889984Z",
     "shell.execute_reply.started": "2025-04-23T00:33:00.884708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are an expert coder with educational skills for beginners. You are able to explain, debbug or generate code in Python, R or bash, and to provide examples of use case if applicable. Please add references to relevant sources if available. If not, do not invent.\n",
       "this is an example of a response:\n",
       "Sure! Hereâ€™s the explanation in plain text format, suitable for Markdown:\n",
       "\n",
       "# Explanation of the Code\n",
       "\n",
       "### Code:\n",
       "```python\n",
       "full_name = lambda first, last: f'Full name: {first.title()} {last.title()}'\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "\n",
       "1. **Lambda Function:**\n",
       "   - The keyword `lambda` is used to create a small, one-line anonymous function (a function without a name).\n",
       "   - It takes two parameters: `first` (for the first name) and `last` (for the last name).\n",
       "\n",
       "2. **String Formatting (`f-string`):**\n",
       "   - `f'Full name: {first.title()} {last.title()}'` is a formatted string (f-string).\n",
       "   - It inserts the values of `first` and `last` into the string while applying `.title()` to capitalize the first letter of each name.\n",
       "\n",
       "3. **Assigning the Function:**\n",
       "   - The lambda function is assigned to the variable `full_name`, so we can use `full_name()` like a regular function.\n",
       "\n",
       "### How to Use It:\n",
       "Now, letâ€™s call this function and see what it does.\n",
       "\n",
       "```python\n",
       "print(full_name(\"john\", \"doe\"))\n",
       "```\n",
       "\n",
       "#### Output:\n",
       "```\n",
       "Full name: John Doe\n",
       "```\n",
       "\n",
       "### What Happens:\n",
       "- `\"john\"` becomes `\"John\"` (because `.title()` capitalizes the first letter).\n",
       "- `\"doe\"` becomes `\"Doe\"`.\n",
       "- The output is `\"Full name: John Doe\"`.\n",
       "\n",
       "### Summary:\n",
       "This is a simple way to create a function that formats a full name while ensuring proper capitalization. You could write the same function using `def` like this:\n",
       "\n",
       "```python\n",
       "def full_name(first, last):\n",
       "    return f'Full name: {first.title()} {last.title()}'\n",
       "```\n",
       "\n",
       "Both versions work the same way, but the `lambda` version is more compact.\n",
       "\n",
       "### Reference(s):\n",
       "To deepen your understanding of the code snippet involving Python's lambda functions here is a resource you might find helpful:\n",
       "\n",
       "Ref. **Python Lambda Functions:**\n",
       "   - The official Python documentation provides an in-depth explanation of lambda expressions, including their syntax and use cases.îˆ†\n",
       "     - [Lambda Expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)\n",
       "\n",
       "```\n",
       "You can copy and paste this into any Markdown file or viewer. Let me know if you need further modifications! ðŸ˜Š\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(system_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fa2506-4b24-4a53-9f3f-500b4cbcb10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:22.227270Z",
     "iopub.status.busy": "2025-04-23T00:33:22.226475Z",
     "iopub.status.idle": "2025-04-23T00:33:31.780527Z",
     "shell.execute_reply": "2025-04-23T00:33:31.779964Z",
     "shell.execute_reply.started": "2025-04-23T00:33:22.227239Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What code do you want me to explain?/n(Press 'Enter' for an example) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please explain what this code does and why:\n",
      "yield from {book.get('author') from book in books if book.get('author')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user question\n",
    "default_question= \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get('author') from book in books if book.get('author')}\n",
    "\"\"\"\n",
    "user_question= str(input(\"What code do you want me to explain?/n(Press 'Enter' for an example)\"))\n",
    "\n",
    "if user_question== '':\n",
    "    question= default_question\n",
    "    print(default_question)\n",
    "else:\n",
    "    question= \"Please explain what this code does and why:\\n\" + user_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6749065-fb8a-4f9f-8297-3cd33abd97bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:38.148166Z",
     "iopub.status.busy": "2025-04-23T00:33:38.147906Z",
     "iopub.status.idle": "2025-04-23T00:33:38.152218Z",
     "shell.execute_reply": "2025-04-23T00:33:38.151723Z",
     "shell.execute_reply.started": "2025-04-23T00:33:38.148143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please explain what this code does and why:\n",
      "yield from {book.get('author') from book in books if book.get('author')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48df06c-edb7-4a05-9e56-910854dad0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:44.176707Z",
     "iopub.status.busy": "2025-04-23T00:33:44.176458Z",
     "iopub.status.idle": "2025-04-23T00:33:49.016946Z",
     "shell.execute_reply": "2025-04-23T00:33:49.016420Z",
     "shell.execute_reply.started": "2025-04-23T00:33:44.176682Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Please enter the number of the model you want to use from the list below:\n",
      "1 GPT-4o Mini\n",
      "2 Llama 3.2\n",
      "3 DeepSeek R1\n",
      "4 Qwen 2.5\n",
      " 4\n"
     ]
    }
   ],
   "source": [
    "# user model\n",
    "model_number= input(\"\"\"\n",
    "Please enter the number of the model you want to use from the list below:\n",
    "1 GPT-4o Mini\n",
    "2 Llama 3.2\n",
    "3 DeepSeek R1\n",
    "4 Qwen 2.5\n",
    "\"\"\")\n",
    "try:\n",
    "    if int(model_number)==1:\n",
    "        model= 'gpt-4o-mini'\n",
    "    elif int(model_number)==2:\n",
    "        model= 'llama3.2'\n",
    "    elif int(model_number)==3:\n",
    "        model= 'deepseek-r1:1.5b'\n",
    "    elif int(model_number)==4:\n",
    "        model= 'qwen2.5:3b'\n",
    "    else:\n",
    "        model= ''\n",
    "        print(\"please provide only a number from the list\")\n",
    "except:\n",
    "    model=''\n",
    "    print(\"Please provide a number or press 'Enter' to finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6e4e5-fb63-4192-bb74-0b015dfedfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fffa6021-d3f8-4855-a694-bed6d651791f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:53.566912Z",
     "iopub.status.busy": "2025-04-23T00:33:53.566587Z",
     "iopub.status.idle": "2025-04-23T00:33:53.570822Z",
     "shell.execute_reply": "2025-04-23T00:33:53.569951Z",
     "shell.execute_reply.started": "2025-04-23T00:33:53.566886Z"
    }
   },
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835374a4-3df5-4f28-82e3-6bc70514df16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T00:33:56.808784Z",
     "iopub.status.busy": "2025-04-23T00:33:56.808090Z",
     "iopub.status.idle": "2025-04-23T00:33:57.273534Z",
     "shell.execute_reply": "2025-04-23T00:33:57.272464Z",
     "shell.execute_reply.started": "2025-04-23T00:33:56.808760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ollama: command not found\n",
      "\n",
      "\n",
      "The following answer will be generated by qwen2.5:3b LLM\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mollama pull \u001b[39m\u001b[38;5;132;01m{model}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThe following answer will be generated by \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m LLM\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(model))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m result= response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m display(Markdown(result))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llms/lib/python3.11/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llms/lib/python3.11/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llms/lib/python3.11/site-packages/ollama/_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "if int(model_number)==1:\n",
    "    openai= OpenAI()\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream= True\n",
    "        )\n",
    "\n",
    "    response = \"\"\n",
    "    print(\"The following answer will be generated by {0} LLM\".format(model))\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "elif int(model_number)==2 or 3 or 4:\n",
    "    !ollama pull {model}\n",
    "    print(\"\\n\\nThe following answer will be generated by {0} LLM\\n\\n\".format(model))\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages = messages)\n",
    "    result= response['message']['content']\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c59b98-5d79-433a-a340-442660414f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms]",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
